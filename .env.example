# =============================================================================
# Data Flight Pipeline - Environment Variables Template
# =============================================================================
# Copy this file to .env and fill in the values for your environment.
# 
# IMPORTANT: Never commit the .env file to version control!
# =============================================================================

# -----------------------------------------------------------------------------
# Airflow Configuration
# -----------------------------------------------------------------------------
# Airflow UID - Run `id -u` to get your user ID
AIRFLOW_UID=50000

# Secret key for Airflow web server (generate with: python -c "import secrets; print(secrets.token_hex(32))")
AIRFLOW__WEBSERVER__SECRET_KEY=your-secret-key-here

# Default Airflow admin credentials
_AIRFLOW_WWW_USER_USERNAME=admin
_AIRFLOW_WWW_USER_PASSWORD=admin

# -----------------------------------------------------------------------------
# PostgreSQL Configuration (Airflow metadata + dbt target)
# -----------------------------------------------------------------------------
POSTGRES_HOST=postgres
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow
POSTGRES_DB=airflow
POSTGRES_PORT=5432
POSTGRES_SCHEMA=public

# -----------------------------------------------------------------------------
# MinIO / S3 Configuration
# -----------------------------------------------------------------------------
# MinIO root credentials (for MinIO admin console)
MINIO_ROOT_USER=minioadmin
MINIO_ROOT_PASSWORD=minioadmin

# MinIO access credentials (for S3 API access from Spark/Airflow)
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin

# MinIO endpoint (use Docker service name for internal, localhost for external)
MINIO_ENDPOINT=http://minio:9000

# -----------------------------------------------------------------------------
# Spark Configuration
# -----------------------------------------------------------------------------
# Spark Master URL (use Docker service name)
SPARK_MASTER_URL=spark://spark-master:7077

# -----------------------------------------------------------------------------
# AviationStack API Configuration
# -----------------------------------------------------------------------------
# Get your API key from: https://aviationstack.com/
AVIATIONSTACK_API_KEY=your-api-key-here
AVIATIONSTACK_BASE_URL=http://api.aviationstack.com/v1
AVIATIONSTACK_LIMIT=100

# -----------------------------------------------------------------------------
# OpenFlights Configuration
# -----------------------------------------------------------------------------
OPENFLIGHTS_BASE_URL=https://raw.githubusercontent.com/jpatokal/openflights/master/data

# -----------------------------------------------------------------------------
# Docker (for stop/start Metabase before/after dbt - avoids DuckDB lock)
# -----------------------------------------------------------------------------
# GID of docker group on host - run `getent group docker | cut -d: -f3` to get it
# Required for Airflow to control Metabase container via Docker socket
DOCKER_GID=999

# -----------------------------------------------------------------------------
# Optional: Caddy Configuration (if using reverse proxy)
# -----------------------------------------------------------------------------
# CADDY_DOMAIN=your-domain.com

# =============================================================================
# Airflow Variables (can also be set via Airflow UI or CLI)
# =============================================================================
# These variables can be configured in Airflow UI under Admin > Variables
# instead of environment variables:
#
# - s3_bucket: flights-data-lake
# - local_raw_path: /opt/airflow/data/raw
# - local_bronze_path: /opt/airflow/data/bronze
# - max_pages: 1
# - aws_conn_id: aws_default
# - spark_executor_cores: 2
# - spark_executor_memory: 2g
# - dag_owner: your-email@example.com

# =============================================================================
# Airflow Connections (must be set via Airflow UI or CLI)
# =============================================================================
# Create these connections in Airflow UI under Admin > Connections:
#
# Connection ID: aws_default
#   Connection Type: Amazon Web Services
#   Extra: {"endpoint_url": "http://minio:9000", "aws_access_key_id": "minioadmin", "aws_secret_access_key": "minioadmin"}
#
# Connection ID: spark_default
#   Connection Type: Spark
#   Host: spark://spark-master
#   Port: 7077
