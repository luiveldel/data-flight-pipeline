FROM apache/airflow:2.10.3-python3.10 AS airflow

USER root

# ---- System deps ----
RUN apt-get update \
  && apt-get install -y --no-install-recommends \
  openjdk-17-jre-headless \
  gcc \
  python3-dev \
  libpq-dev \
  curl \
  procps \
  && apt-get autoremove -yqq --purge \
  && apt-get clean \
  && rm -rf /var/lib/apt/lists/*

# Set JAVA_HOME
ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
ENV PATH="${JAVA_HOME}/bin:${PATH}"

# Pre-download Spark JARs for S3 connectivity
RUN mkdir -p /opt/airflow/jars && \
  curl -L -o /opt/airflow/jars/hadoop-aws-3.3.4.jar \
    https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar && \
  curl -L -o /opt/airflow/jars/aws-java-sdk-bundle-1.12.262.jar \
    https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar && \
  chown -R airflow:0 /opt/airflow/jars

# install uv
COPY --from=ghcr.io/astral-sh/uv:0.5.6 /uv /usr/local/bin/uv

# needed for pip/uv installs afterwards
USER airflow
WORKDIR /opt/airflow

# Create virtualenv and install dependencies
ENV PATH="/opt/airflow/.venv/bin:${PATH}"

# use cache for performance
COPY --chown=airflow:0 pyproject.toml uv.lock ./

# Remove --no-cache to allow uv to use its internal caching mechanism
RUN uv sync --frozen --no-dev

# Copy code
COPY --chown=airflow:0 include /opt/airflow/include
COPY --chown=airflow:0 spark_jobs /opt/airflow/spark_jobs
COPY --chown=airflow:0 dags /opt/airflow/dags
COPY --chown=airflow:0 dbt_transform /opt/airflow/dbt_transform
